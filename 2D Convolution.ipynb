{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy loader method to load numpy arrays into the Pytorch ImageFolder dataset\n",
    "import numpy as np\n",
    "\n",
    "def my_numpy_loader(filename):\n",
    "    return np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import visdom\n",
    "import numpy as np\n",
    "viz = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset DatasetFolder\n",
      "    Number of datapoints: 982\n",
      "    Root Location: 2D/train\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n",
      "Dataset DatasetFolder\n",
      "    Number of datapoints: 422\n",
      "    Root Location: 2D/test\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n",
      "Epoch [1/15], Step [1/10], Loss: 0.7883\n",
      "Epoch [1/15], Step [2/10], Loss: 6.3802\n",
      "Epoch [1/15], Step [3/10], Loss: 4.0139\n",
      "Epoch [1/15], Step [4/10], Loss: 0.6660\n",
      "Epoch [1/15], Step [5/10], Loss: 2.1404\n",
      "Epoch [1/15], Step [6/10], Loss: 0.6512\n",
      "Epoch [1/15], Step [7/10], Loss: 1.3081\n",
      "Epoch [1/15], Step [8/10], Loss: 1.5958\n",
      "Epoch [1/15], Step [9/10], Loss: 1.4178\n",
      "Epoch [1/15], Step [10/10], Loss: 0.6415\n",
      "Test Accuracy of the model on the 212 test images: 52.60663507109005 %\n",
      "Epoch [2/15], Step [1/10], Loss: 0.6858\n",
      "Epoch [2/15], Step [2/10], Loss: 0.6992\n",
      "Epoch [2/15], Step [3/10], Loss: 0.6975\n",
      "Epoch [2/15], Step [4/10], Loss: 0.7209\n",
      "Epoch [2/15], Step [5/10], Loss: 0.7145\n",
      "Epoch [2/15], Step [6/10], Loss: 0.6739\n",
      "Epoch [2/15], Step [7/10], Loss: 0.6631\n",
      "Epoch [2/15], Step [8/10], Loss: 0.5721\n",
      "Epoch [2/15], Step [9/10], Loss: 0.5840\n",
      "Epoch [2/15], Step [10/10], Loss: 0.4817\n",
      "Test Accuracy of the model on the 212 test images: 62.322274881516584 %\n",
      "Epoch [3/15], Step [1/10], Loss: 0.7031\n",
      "Epoch [3/15], Step [2/10], Loss: 0.6435\n",
      "Epoch [3/15], Step [3/10], Loss: 0.6640\n",
      "Epoch [3/15], Step [4/10], Loss: 0.5318\n",
      "Epoch [3/15], Step [5/10], Loss: 0.5531\n",
      "Epoch [3/15], Step [6/10], Loss: 0.4924\n",
      "Epoch [3/15], Step [7/10], Loss: 0.5264\n",
      "Epoch [3/15], Step [8/10], Loss: 0.5628\n",
      "Epoch [3/15], Step [9/10], Loss: 0.5374\n",
      "Epoch [3/15], Step [10/10], Loss: 0.5873\n",
      "Test Accuracy of the model on the 212 test images: 80.33175355450237 %\n",
      "Epoch [4/15], Step [1/10], Loss: 0.5989\n",
      "Epoch [4/15], Step [2/10], Loss: 0.5026\n",
      "Epoch [4/15], Step [3/10], Loss: 0.5106\n",
      "Epoch [4/15], Step [4/10], Loss: 0.4306\n",
      "Epoch [4/15], Step [5/10], Loss: 0.4128\n",
      "Epoch [4/15], Step [6/10], Loss: 0.4948\n",
      "Epoch [4/15], Step [7/10], Loss: 0.4806\n",
      "Epoch [4/15], Step [8/10], Loss: 0.5102\n",
      "Epoch [4/15], Step [9/10], Loss: 0.5844\n",
      "Epoch [4/15], Step [10/10], Loss: 0.4711\n",
      "Test Accuracy of the model on the 212 test images: 76.06635071090048 %\n",
      "Epoch [5/15], Step [1/10], Loss: 0.4601\n",
      "Epoch [5/15], Step [2/10], Loss: 0.5657\n",
      "Epoch [5/15], Step [3/10], Loss: 0.4750\n",
      "Epoch [5/15], Step [4/10], Loss: 0.5144\n",
      "Epoch [5/15], Step [5/10], Loss: 0.4376\n",
      "Epoch [5/15], Step [6/10], Loss: 0.4295\n",
      "Epoch [5/15], Step [7/10], Loss: 0.4239\n",
      "Epoch [5/15], Step [8/10], Loss: 0.4322\n",
      "Epoch [5/15], Step [9/10], Loss: 0.5327\n",
      "Epoch [5/15], Step [10/10], Loss: 0.5532\n",
      "Test Accuracy of the model on the 212 test images: 80.09478672985782 %\n",
      "Epoch [6/15], Step [1/10], Loss: 0.4398\n",
      "Epoch [6/15], Step [2/10], Loss: 0.4156\n",
      "Epoch [6/15], Step [3/10], Loss: 0.3738\n",
      "Epoch [6/15], Step [4/10], Loss: 0.4422\n",
      "Epoch [6/15], Step [5/10], Loss: 0.3951\n",
      "Epoch [6/15], Step [6/10], Loss: 0.4372\n",
      "Epoch [6/15], Step [7/10], Loss: 0.4462\n",
      "Epoch [6/15], Step [8/10], Loss: 0.4896\n",
      "Epoch [6/15], Step [9/10], Loss: 0.4740\n",
      "Epoch [6/15], Step [10/10], Loss: 0.4246\n",
      "Test Accuracy of the model on the 212 test images: 80.56872037914692 %\n",
      "Epoch [7/15], Step [1/10], Loss: 0.3872\n",
      "Epoch [7/15], Step [2/10], Loss: 0.4124\n",
      "Epoch [7/15], Step [3/10], Loss: 0.3216\n",
      "Epoch [7/15], Step [4/10], Loss: 0.5177\n",
      "Epoch [7/15], Step [5/10], Loss: 0.4391\n",
      "Epoch [7/15], Step [6/10], Loss: 0.4443\n",
      "Epoch [7/15], Step [7/10], Loss: 0.4538\n",
      "Epoch [7/15], Step [8/10], Loss: 0.3627\n",
      "Epoch [7/15], Step [9/10], Loss: 0.3659\n",
      "Epoch [7/15], Step [10/10], Loss: 0.4688\n",
      "Test Accuracy of the model on the 212 test images: 82.46445497630332 %\n",
      "Epoch [8/15], Step [1/10], Loss: 0.4072\n",
      "Epoch [8/15], Step [2/10], Loss: 0.3976\n",
      "Epoch [8/15], Step [3/10], Loss: 0.3661\n",
      "Epoch [8/15], Step [4/10], Loss: 0.3297\n",
      "Epoch [8/15], Step [5/10], Loss: 0.3586\n",
      "Epoch [8/15], Step [6/10], Loss: 0.5075\n",
      "Epoch [8/15], Step [7/10], Loss: 0.3282\n",
      "Epoch [8/15], Step [8/10], Loss: 0.4425\n",
      "Epoch [8/15], Step [9/10], Loss: 0.3562\n",
      "Epoch [8/15], Step [10/10], Loss: 0.4063\n",
      "Test Accuracy of the model on the 212 test images: 86.72985781990522 %\n",
      "Epoch [9/15], Step [1/10], Loss: 0.3564\n",
      "Epoch [9/15], Step [2/10], Loss: 0.4508\n",
      "Epoch [9/15], Step [3/10], Loss: 0.3539\n",
      "Epoch [9/15], Step [4/10], Loss: 0.3430\n",
      "Epoch [9/15], Step [5/10], Loss: 0.3989\n",
      "Epoch [9/15], Step [6/10], Loss: 0.3681\n",
      "Epoch [9/15], Step [7/10], Loss: 0.3707\n",
      "Epoch [9/15], Step [8/10], Loss: 0.2973\n",
      "Epoch [9/15], Step [9/10], Loss: 0.3440\n",
      "Epoch [9/15], Step [10/10], Loss: 0.3158\n",
      "Test Accuracy of the model on the 212 test images: 87.44075829383887 %\n",
      "Epoch [10/15], Step [1/10], Loss: 0.3912\n",
      "Epoch [10/15], Step [2/10], Loss: 0.2574\n",
      "Epoch [10/15], Step [3/10], Loss: 0.3374\n",
      "Epoch [10/15], Step [4/10], Loss: 0.3311\n",
      "Epoch [10/15], Step [5/10], Loss: 0.3009\n",
      "Epoch [10/15], Step [6/10], Loss: 0.2870\n",
      "Epoch [10/15], Step [7/10], Loss: 0.4376\n",
      "Epoch [10/15], Step [8/10], Loss: 0.3165\n",
      "Epoch [10/15], Step [9/10], Loss: 0.2670\n",
      "Epoch [10/15], Step [10/10], Loss: 0.3269\n",
      "Test Accuracy of the model on the 212 test images: 88.86255924170617 %\n",
      "Epoch [11/15], Step [1/10], Loss: 0.3334\n",
      "Epoch [11/15], Step [2/10], Loss: 0.3526\n",
      "Epoch [11/15], Step [3/10], Loss: 0.3667\n",
      "Epoch [11/15], Step [4/10], Loss: 0.2537\n",
      "Epoch [11/15], Step [5/10], Loss: 0.2655\n",
      "Epoch [11/15], Step [6/10], Loss: 0.2473\n",
      "Epoch [11/15], Step [7/10], Loss: 0.2866\n",
      "Epoch [11/15], Step [8/10], Loss: 0.2191\n",
      "Epoch [11/15], Step [9/10], Loss: 0.2693\n",
      "Epoch [11/15], Step [10/10], Loss: 0.3053\n",
      "Test Accuracy of the model on the 212 test images: 90.99526066350711 %\n",
      "Epoch [12/15], Step [1/10], Loss: 0.2759\n",
      "Epoch [12/15], Step [2/10], Loss: 0.2042\n",
      "Epoch [12/15], Step [3/10], Loss: 0.3293\n",
      "Epoch [12/15], Step [4/10], Loss: 0.3129\n",
      "Epoch [12/15], Step [5/10], Loss: 0.4553\n",
      "Epoch [12/15], Step [6/10], Loss: 0.1765\n",
      "Epoch [12/15], Step [7/10], Loss: 0.3926\n",
      "Epoch [12/15], Step [8/10], Loss: 0.2323\n",
      "Epoch [12/15], Step [9/10], Loss: 0.3443\n",
      "Epoch [12/15], Step [10/10], Loss: 0.3113\n",
      "Test Accuracy of the model on the 212 test images: 92.18009478672985 %\n",
      "Epoch [13/15], Step [1/10], Loss: 0.3053\n",
      "Epoch [13/15], Step [2/10], Loss: 0.2428\n",
      "Epoch [13/15], Step [3/10], Loss: 0.2275\n",
      "Epoch [13/15], Step [4/10], Loss: 0.2088\n",
      "Epoch [13/15], Step [5/10], Loss: 0.1879\n",
      "Epoch [13/15], Step [6/10], Loss: 0.2457\n",
      "Epoch [13/15], Step [7/10], Loss: 0.2971\n",
      "Epoch [13/15], Step [8/10], Loss: 0.2793\n",
      "Epoch [13/15], Step [9/10], Loss: 0.3859\n",
      "Epoch [13/15], Step [10/10], Loss: 0.2804\n",
      "Test Accuracy of the model on the 212 test images: 91.70616113744076 %\n",
      "Epoch [14/15], Step [1/10], Loss: 0.2862\n",
      "Epoch [14/15], Step [2/10], Loss: 0.2405\n",
      "Epoch [14/15], Step [3/10], Loss: 0.2991\n",
      "Epoch [14/15], Step [4/10], Loss: 0.2282\n",
      "Epoch [14/15], Step [5/10], Loss: 0.2239\n",
      "Epoch [14/15], Step [6/10], Loss: 0.3362\n",
      "Epoch [14/15], Step [7/10], Loss: 0.2096\n",
      "Epoch [14/15], Step [8/10], Loss: 0.2821\n",
      "Epoch [14/15], Step [9/10], Loss: 0.2606\n",
      "Epoch [14/15], Step [10/10], Loss: 0.2572\n",
      "Test Accuracy of the model on the 212 test images: 92.18009478672985 %\n",
      "Epoch [15/15], Step [1/10], Loss: 0.2980\n",
      "Epoch [15/15], Step [2/10], Loss: 0.2908\n",
      "Epoch [15/15], Step [3/10], Loss: 0.1932\n",
      "Epoch [15/15], Step [4/10], Loss: 0.1872\n",
      "Epoch [15/15], Step [5/10], Loss: 0.2526\n",
      "Epoch [15/15], Step [6/10], Loss: 0.2566\n",
      "Epoch [15/15], Step [7/10], Loss: 0.2916\n",
      "Epoch [15/15], Step [8/10], Loss: 0.2362\n",
      "Epoch [15/15], Step [9/10], Loss: 0.2698\n",
      "Epoch [15/15], Step [10/10], Loss: 0.2628\n",
      "Test Accuracy of the model on the 212 test images: 92.41706161137441 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import visdom\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Visdom Configuration\n",
    "viz = visdom.Visdom()\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 15\n",
    "num_classes = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dicom dataset\n",
    "train_dataset = torchvision.datasets.DatasetFolder(root='2D/train', \n",
    "                                           transform=transforms.ToTensor(), loader=my_numpy_loader, extensions=\"npy\")\n",
    "\n",
    "test_dataset = torchvision.datasets.DatasetFolder(root='2D/test', \n",
    "                                          transform=transforms.ToTensor(), loader=my_numpy_loader, extensions=\"npy\")\n",
    "# print(train_dataset)\n",
    "# print(test_dataset)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet,self).__init__()\n",
    "        #Convo 1\n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5,stride=1,padding=2)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Maxpool_1\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        #Convo_2\n",
    "        self.cnn2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5,stride=1,padding=2)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Maxpool_2\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        #Linear Layer\n",
    "        self.fc1=nn.Linear(8192,1024)\n",
    "        self.fc2=nn.Linear(1024,32)\n",
    "        self.fc3=nn.Linear(32,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #Convo_1\n",
    "        out=self.cnn1(x)\n",
    "        #print(out.shape)\n",
    "        out=self.batch_norm1(out)\n",
    "        #print(out.shape)\n",
    "        out=self.relu1(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        #Max_pool1\n",
    "        out=self.maxpool1(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        #Convo_2\n",
    "        out=self.cnn2(out)\n",
    "        #print(out.shape)\n",
    "        out=self.batch_norm2(out)\n",
    "        #print(out.shape)\n",
    "        out=self.relu2(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        #Max_pool2\n",
    "        out=self.maxpool2(out)\n",
    "        #print(out.shape)\n",
    "        out=out.view(out.size(0),-1)#Flattening out \n",
    "        #print(out.shape)\n",
    "        \n",
    "        #Linear Layers\n",
    "        out=self.fc1(out)\n",
    "        #print(out.shape)\n",
    "        out=self.fc2(out)\n",
    "        #print(out.shape)\n",
    "        out=self.fc3(out)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device).double()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_window = vis.line(\n",
    "    Y=torch.zeros((1)).cpu(),\n",
    "    X=torch.zeros((1)).cpu(),\n",
    "    opts=dict(xlabel='Epoch',ylabel='Loss',title='Training Loss'))\n",
    "\n",
    "accuracy_window = vis.line(\n",
    "    Y=torch.zeros((1)).cpu(),\n",
    "    X=torch.zeros((1)).cpu(),\n",
    "    opts=dict(xlabel='Epoch',ylabel='Accuracy [%]',title='Network Accuracy'))\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        loss_array = np.array([loss.item()])\n",
    "        viz.line(X=torch.ones((1,1)).cpu()*counter, Y=torch.Tensor([loss]).unsqueeze(0),win=loss_window,update='append')\n",
    "        counter = counter + 1\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Test Accuracy of the model on the 212 test images: {} %'.format(100 * correct / total))\n",
    "        acc = 100 * correct / total\n",
    "        viz.line(X=torch.ones((1,1)).cpu()*counter, Y=torch.Tensor([acc]).unsqueeze(0),win=accuracy_window,update='append')\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdom import Visdom\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asr)",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
